{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import cos, sin, exp, pi\n",
    "from itertools import combinations_with_replacement\n",
    "from functools import reduce\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "\n",
    "from layers.activation.tangent import Tangent\n",
    "from layers.base.affine import Affine\n",
    "from layers.loss.squared_error import SquaredError\n",
    "from layers.optimizer.adam import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "\n",
    "poly = lambda x:x*x\n",
    "sinusoidal = lambda x:sin(pi*x)\n",
    "absolute = lambda x:abs(x)\n",
    "step = lambda x:0.5 * (np.sign(x)+1)\n",
    "funcs = [poly, sinusoidal, absolute, step]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Tangent1'] = Tangent\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SquaredError()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return self.lastLayer.forward(x, t)\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W:self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        #forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward()\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 フィードフォワードネットワーク関数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "N_train = 40\n",
    "N_test = 100\n",
    "x_train = 2 * np.random.rand(N_train) - 1\n",
    "x_test = np.linspace(-1, 1, N_test)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_test = x_test.reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-31-4d37e0cdd1a8>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mhidden_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0moutput_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAdam\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[0mt_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mN_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Adam' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "max_epoch = 20000\n",
    "input_size = 1\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "optimizer = Adam()\n",
    "for i, func in zip(range(1, 5), funcs):\n",
    "    t_train = func(x_train) + np.random.normal(0, 0.1, (N_train, 1))\n",
    "    t_test = func(x_test)\n",
    "    model = TwoLayerNet(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        grad = model.gradient(x_train, t_train)\n",
    "        optimizer.update(model.params, grad)\n",
    "        loss = model.loss(x_train, t_train)\n",
    "    y = model.predict(x_test)\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.plot(x_test, y)\n",
    "    plt.plot(x_test, t_test)\n",
    "    plt.scatter(x_train, t_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 ニューラルネットワークの正則化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_train = 10\n",
    "N_test = 100\n",
    "x_train = np.linspace(-1, 1, N_train)\n",
    "x_test = np.linspace(-1, 1, N_test)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_test = x_test.reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "max_epoch = 10000\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "t_train = sinusoidal(x_train) + np.random.normal(0, 0.2, (N_train, 1))\n",
    "t_test = sinusoidal(x_test)\n",
    "for i, hidden_size in zip(range(1, 4), [1, 3, 10]):\n",
    "    model = TwoLayerNet(input_size, hidden_size, output_size)\n",
    "    optimizer = Adam()\n",
    "    for epoch in range(max_epoch):\n",
    "        grad = model.gradient(x_train, t_train)\n",
    "        optimizer.update(model.params, grad)\n",
    "        loss = model.loss(x_train, t_train)\n",
    "    y = model.predict(x_test)\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.plot(x_test, y, label=f'M={hidden_size}')\n",
    "    plt.scatter(x_train, t_train)\n",
    "    plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "荷重減衰"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RegularizedTwoLayerNet(TwoLayerNet):\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_decay_lambda=0):\n",
    "        super().__init__(input_size, hidden_size, output_size)\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        reg_term = 0\n",
    "        for i in range(1, 3):\n",
    "            W = self.params[f'W{i}']\n",
    "            reg_term += self.weight_decay_lambda / 2 * W.T @ W\n",
    "        return self.lastLayer.forward(y, t) + reg_term\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        #forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW + \\\n",
    "                      self.weight_decay_lambda * self.layers['Affine1'].W\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW + \\\n",
    "                      self.weight_decay_lambda * self.layers['Affine2'].W\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "max_epoch = 10000\n",
    "input_size = 1\n",
    "hidden_size = 30\n",
    "output_size = 1\n",
    "\n",
    "t_train = sinusoidal(x_train) + np.random.normal(0, 0.2, (N_train, 1))\n",
    "t_test = sinusoidal(x_test)\n",
    "for i, weight_decay_lambda in zip(range(1, 4), [0.00001, 0.001, 0.1]):\n",
    "    model = RegularizedTwoLayerNet(input_size, hidden_size, output_size, weight_decay_lambda)\n",
    "    optimizer = Adam()\n",
    "    for epoch in range(max_epoch):\n",
    "        grad = model.gradient(x_train, t_train)\n",
    "        optimizer.update(model.params, grad)\n",
    "        loss = model.loss(x_train, t_train)\n",
    "    y = model.predict(x_test)\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.plot(x_test, y, label=f'weight decay lambda={weight_decay_lambda}')\n",
    "    plt.scatter(x_train, t_train)\n",
    "    plt.xlim(-1.05, 1.05)\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.6 混合密度ネットワーク"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "N_train = 300\n",
    "N_test = 300\n",
    "batch_size = 80\n",
    "\n",
    "max_epoch = 100000\n",
    "input_size = 1\n",
    "hidden_size = 6\n",
    "output_size = 1\n",
    "\n",
    "x_train = np.random.rand(N_train).reshape(-1, 1)\n",
    "x_test = np.linspace(0, 1, N_test).reshape(-1, 1)\n",
    "t_train = x_train + 0.3 * sinusoidal(2*x_train) + np.random.normal(0, 0.1, (N_train, 1))\n",
    "t_test = x_test + sinusoidal(2*x_test)\n",
    "model = TwoLayerNet(input_size, hidden_size, output_size)\n",
    "optimizer = Adam()\n",
    "for epoch in range(max_epoch):\n",
    "    index = np.random.choice(N_train, batch_size, replace=True)\n",
    "    grad = model.gradient(x_train, t_train)\n",
    "    optimizer.update(model.params, grad)\n",
    "y = model.predict(x_test)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_test, y)\n",
    "plt.scatter(x_train, t_train)\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "\n",
    "\n",
    "t_train = np.random.rand(N_train).reshape(-1, 1)\n",
    "t_test = np.linspace(0, 1, N_test).reshape(-1, 1)\n",
    "x_train = t_train + 0.3 * sinusoidal(2*t_train) + np.random.normal(0, 0.1, (N_train, 1))\n",
    "x_test = t_test + sinusoidal(2*t_test)\n",
    "model = TwoLayerNet(input_size, hidden_size, output_size)\n",
    "optimizer = Adam()\n",
    "for epoch in range(max_epoch):\n",
    "    index = np.random.choice(N_train, batch_size, replace=True)\n",
    "    grad = model.gradient(x_train, t_train)\n",
    "    optimizer.update(model.params, grad)\n",
    "y = model.predict(x_test)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_test, y)\n",
    "plt.scatter(x_train, t_train)\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 混合密度ネットワークのコスト関数\n",
    "class MixtureGaussian:\n",
    "    def __init__(self, components_size):\n",
    "        self.components_size = components_size\n",
    "        self.t = None\n",
    "        self.sigma = None\n",
    "        self.weight = None\n",
    "        self.mu = None\n",
    "\n",
    "    def _gaussian(self, t):\n",
    "        return np.exp(-0.5 * (self.mu - t) ** 2 / np.square(self.sigma))\\\n",
    "               / np.sqrt(2 * np.pi * np.square(self.sigma))\n",
    "\n",
    "    def activate(self, x):\n",
    "        a_sigma, a_weight, a_mu = np.split(\n",
    "            x, [self.components_size, self.components_size*2], axis=1)\n",
    "\n",
    "        self.sigma = exp(a_sigma)\n",
    "\n",
    "        self.weight = exp(a_weight - np.max(a_weight, 1, keepdims=True))\n",
    "        self.weight /= np.sum(self.weight, axis=1, keepdims=True)\n",
    "\n",
    "        self.mu = a_mu\n",
    "\n",
    "        return self.sigma, self.weight, a_mu\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.sigma, self.weight, self.mu = self.activate(x)\n",
    "\n",
    "        gaussian = self._gaussian(self.t)\n",
    "\n",
    "        return -np.sum(np.log(np.sum(self.weight * gaussian, axis=1)))\n",
    "\n",
    "    def backward(self):\n",
    "        gamma = self.weight * self._gaussian(self.t)\n",
    "        gamma /= np.sum(gamma, axis=1, keepdims=True)\n",
    "\n",
    "        delta_sigma = gamma * (1 - (self.mu - self.t)**2 / (self.sigma**2))\n",
    "        delta_weight = self.weight - gamma\n",
    "        delta_mu = gamma * (self.mu - self.t) / (self.sigma**2)\n",
    "\n",
    "        delta = np.hstack([delta_sigma, delta_weight, delta_mu])\n",
    "        return delta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MixtureDensityNet(TwoLayerNet):\n",
    "    def __init__(self, input_size, hidden_size, output_size, components_size):\n",
    "        super().__init__(input_size, hidden_size, output_size)\n",
    "\n",
    "        self.lastLayer = MixtureGaussian(components_size)\n",
    "\n",
    "    def predict(self, x, t):\n",
    "        self.t = t\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        sigma, weight, mu = self.lastLayer.activate(x)\n",
    "        gauss = self.lastLayer._gaussian(self.t)\n",
    "        prob = np.sum(weight * gauss, axis=1)\n",
    "        return prob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_epoch = 1000000\n",
    "input_size = 1\n",
    "hidden_size = 6\n",
    "output_size = 9\n",
    "components_size = 3\n",
    "\n",
    "y_test = np.linspace(t_test.min(), t_test.max(), N_test)\n",
    "X_test, Y_test = np.meshgrid(x_test, y_test)\n",
    "test = np.array([X_test, Y_test]).transpose(1, 2, 0).reshape(-1, 2)\n",
    "\n",
    "model = MixtureDensityNet(input_size, hidden_size, output_size, components_size)\n",
    "optimizer = Adam()\n",
    "for epoch in range(max_epoch):\n",
    "    index = np.random.choice(N_train, batch_size, replace=True)\n",
    "    grad = model.gradient(x_train[index], t_train[index])\n",
    "    optimizer.update(model.params, grad)\n",
    "prob = model.predict(test[:, 0].reshape(-1, 1), test[:, 1].reshape(-1, 1))\n",
    "Prob = prob.reshape(N_test, N_test)\n",
    "\n",
    "weight = model.lastLayer.weight\n",
    "mu = model.lastLayer.mu\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x_test, weight[:N_test, 0], color='blue')\n",
    "plt.plot(x_test, weight[:N_test, 1], color='red')\n",
    "plt.plot(x_test, weight[:N_test, 2], color='green')\n",
    "plt.title('weights')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x_test, mu[:N_test, 0], color='blue')\n",
    "plt.plot(x_test, mu[:N_test, 1], color='red')\n",
    "plt.plot(x_test, mu[:N_test, 2], color='green')\n",
    "plt.title('means')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "levels = np.exp(np.linspace(0, np.log(prob.max()), 20))\n",
    "levels[0] = 0\n",
    "plt.contourf(X_test, Y_test, Prob, levels, alpha=0.5)\n",
    "plt.colorbar()\n",
    "plt.scatter(x_train, t_train)\n",
    "plt.xlim(-0.0, 1.0)\n",
    "plt.ylim(-0.0, 1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}